# KNN å‘é‡ç´¢å¼•å»ºç½®æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è©³ç´°èªªæ˜å¦‚ä½•å»ºç«‹å’Œç®¡ç† OpenSearch KNN å‘é‡ç´¢å¼•ï¼ŒåŒ…æ‹¬ç´¢å¼•è¨­è¨ˆã€æ‰¹æ¬¡è¼‰å…¥ã€åƒæ•¸èª¿æ•´å’Œç¶­è­·ç­–ç•¥ã€‚

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å»ºç«‹åŸºæœ¬ç´¢å¼•

```bash
# ä½¿ç”¨é è¨­åƒæ•¸å»ºç«‹ç´¢å¼•
python scripts/build_knn_index.py --load-sample

# æŒ‡å®šç´¢å¼•åç¨±å’Œç¶­åº¦
python scripts/build_knn_index.py \
    --index-name my_knowledge_base \
    --dimension 768
```

### 2. è‡ªè¨‚ HNSW åƒæ•¸

```bash
# é«˜ç²¾åº¦é…ç½®
python scripts/build_knn_index.py \
    --hnsw-m 48 \
    --hnsw-ef-construction 512 \
    --hnsw-ef-search 200

# å¹³è¡¡é…ç½®
python scripts/build_knn_index.py \
    --hnsw-m 16 \
    --hnsw-ef-construction 128 \
    --hnsw-ef-search 100
```

## ğŸ“ ç´¢å¼•è¨­è¨ˆ

### æ˜ å°„çµæ§‹

```json
{
  "mappings": {
    "properties": {
      "doc_id": {
        "type": "keyword"
      },
      "title": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "content": {
        "type": "text",
        "analyzer": "standard"
      },
      "embedding": {
        "type": "knn_vector",
        "dimension": 768,
        "method": {
          "name": "hnsw",
          "space_type": "l2",
          "engine": "nmslib",
          "parameters": {
            "ef_construction": 128,
            "m": 16
          }
        }
      },
      "tags": {
        "type": "keyword"
      },
      "category": {
        "type": "keyword"
      },
      "metadata": {
        "type": "object"
      },
      "created_at": {
        "type": "date"
      }
    }
  }
}
```

### æ¬„ä½èªªæ˜

| æ¬„ä½ | é¡å‹ | ç”¨é€” | ç´¢å¼•ç­–ç•¥ |
|------|------|------|----------|
| doc_id | keyword | å”¯ä¸€è­˜åˆ¥ç¢¼ | ç²¾ç¢ºåŒ¹é… |
| title | text/keyword | æ–‡ä»¶æ¨™é¡Œ | å…¨æ–‡æœå°‹ + ç²¾ç¢ºåŒ¹é… |
| content | text | æ–‡ä»¶å…§å®¹ | å…¨æ–‡æœå°‹ |
| embedding | knn_vector | å‘é‡è¡¨ç¤º | HNSW ç´¢å¼• |
| tags | keyword | æ¨™ç±¤åˆ†é¡ | ç²¾ç¢ºåŒ¹é…ã€èšåˆ |
| category | keyword | æ–‡ä»¶é¡åˆ¥ | éæ¿¾ã€èšåˆ |
| metadata | object | æ“´å±•è³‡è¨Š | å‹•æ…‹æ˜ å°„ |

## ğŸ”§ ç¨‹å¼åŒ–ç´¢å¼•å»ºç«‹

### åŸºæœ¬ç¯„ä¾‹

```python
from scripts.build_knn_index import KNNIndexBuilder

# å»ºç«‹ç´¢å¼•å»ºç½®å™¨
builder = KNNIndexBuilder(
    index_name="my_knowledge_base",
    embedding_dim=768,
    model_name="models/embedding-001"
)

# å»ºç«‹ç´¢å¼•
result = builder.create_knn_index(
    hnsw_m=16,
    hnsw_ef_construction=128,
    hnsw_ef_search=100,
    space_type="l2"
)
```

### æ‰¹æ¬¡ç´¢å¼•æ–‡ä»¶

```python
# æº–å‚™æ–‡ä»¶
documents = [
    {
        "doc_id": "DOC001",
        "title": "Apache æ•ˆèƒ½å„ªåŒ–æŒ‡å—",
        "content": "è©³ç´°çš„ Apache é…ç½®å„ªåŒ–å…§å®¹...",
        "tags": ["apache", "performance"],
        "category": "tutorial"
    },
    # æ›´å¤šæ–‡ä»¶...
]

# æ‰¹æ¬¡ç´¢å¼•
result = await builder.bulk_index_documents(
    documents=documents,
    batch_size=100
)

print(f"æˆåŠŸç´¢å¼•: {result['success']} å€‹æ–‡ä»¶")
print(f"å¤±æ•—: {result['failed']} å€‹æ–‡ä»¶")
```

### å–®ä¸€æ–‡ä»¶ç´¢å¼•

```python
# ç´¢å¼•å–®ä¸€æ–‡ä»¶
await builder.index_document(
    doc_id="DOC002",
    title="MySQL ç´¢å¼•å„ªåŒ–",
    content="MySQL ç´¢å¼•å»ºç«‹å’Œå„ªåŒ–çš„æœ€ä½³å¯¦è¸...",
    tags=["mysql", "index", "optimization"],
    category="best_practice",
    metadata={
        "author": "DevOps Team",
        "version": "1.0",
        "last_updated": "2024-01-28"
    }
)
```

## ğŸ¯ Embedding æ¨¡å‹é…ç½®

### æ”¯æ´çš„æ¨¡å‹

```python
from src.config.embedding_config import EMBEDDING_MODELS

# æŸ¥çœ‹æ”¯æ´çš„æ¨¡å‹
for model_key, config in EMBEDDING_MODELS.items():
    print(f"{model_key}:")
    print(f"  ç¶­åº¦: {config.dimension}")
    print(f"  é¡å‹: {config.model_type}")
```

### åˆ‡æ›æ¨¡å‹

```python
# ä½¿ç”¨ OpenAI æ¨¡å‹
builder_openai = KNNIndexBuilder(
    index_name="kb_openai",
    embedding_dim=1536,
    model_name="text-embedding-ada-002"
)

# ä½¿ç”¨ HuggingFace æ¨¡å‹
builder_hf = KNNIndexBuilder(
    index_name="kb_hf",
    embedding_dim=384,
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

### ç¶­åº¦ä¸€è‡´æ€§æª¢æŸ¥

```python
from src.config.embedding_config import validate_embedding_dimension

# ç”Ÿæˆ embedding
embedding = await embeddings.aembed_query("æ¸¬è©¦æ–‡å­—")

# é©—è­‰ç¶­åº¦
try:
    validate_embedding_dimension(embedding, "gemini-embedding-001")
    print("ç¶­åº¦æª¢æŸ¥é€šé")
except ValueError as e:
    print(f"ç¶­åº¦éŒ¯èª¤: {e}")
```

## ğŸ“Š ç´¢å¼•ç®¡ç†

### æŸ¥çœ‹ç´¢å¼•çµ±è¨ˆ

```python
# å–å¾—ç´¢å¼•çµ±è¨ˆ
stats = builder.get_index_stats()

print(f"ç´¢å¼•åç¨±: {stats['index_name']}")
print(f"æ–‡ä»¶æ•¸é‡: {stats['document_count']}")
print(f"ç´¢å¼•å¤§å°: {stats['size_in_bytes'] / 1024 / 1024:.2f} MB")
print(f"å‘é‡ç¶­åº¦: {stats['embedding_dimension']}")
print(f"HNSW ef_search: {stats['ef_search']}")
```

### ç´¢å¼•å¥åº·æª¢æŸ¥

```bash
# æª¢æŸ¥ç´¢å¼•å¥åº·ç‹€æ…‹
curl -X GET "localhost:9200/_cluster/health/my_knowledge_base?pretty"

# æŸ¥çœ‹ç´¢å¼•è¨­å®š
curl -X GET "localhost:9200/my_knowledge_base/_settings?pretty"

# æŸ¥çœ‹æ˜ å°„
curl -X GET "localhost:9200/my_knowledge_base/_mapping?pretty"
```

### æ›´æ–°ç´¢å¼•è¨­å®š

```python
# æ›´æ–°æœå°‹åƒæ•¸
client.indices.put_settings(
    index=index_name,
    body={
        "index": {
            "knn.algo_param.ef_search": 150
        }
    }
)
```

## ğŸ”„ è³‡æ–™æ›´æ–°ç­–ç•¥

### å¢é‡æ›´æ–°

```python
async def incremental_update(new_documents):
    """å¢é‡æ›´æ–°ç´¢å¼•"""
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    existing_ids = set()
    for doc in new_documents:
        try:
            client.get(index=index_name, id=doc['doc_id'])
            existing_ids.add(doc['doc_id'])
        except:
            pass
    
    # åˆ†é›¢æ–°æ–‡ä»¶å’Œæ›´æ–°æ–‡ä»¶
    new_docs = [d for d in new_documents if d['doc_id'] not in existing_ids]
    update_docs = [d for d in new_documents if d['doc_id'] in existing_ids]
    
    # ç´¢å¼•æ–°æ–‡ä»¶
    if new_docs:
        await builder.bulk_index_documents(new_docs)
    
    # æ›´æ–°ç¾æœ‰æ–‡ä»¶
    for doc in update_docs:
        await builder.index_document(**doc)
```

### é‡å»ºç´¢å¼•

```python
async def rebuild_index(old_index, new_index):
    """é‡å»ºç´¢å¼•ï¼ˆé›¶åœæ©Ÿï¼‰"""
    # 1. å»ºç«‹æ–°ç´¢å¼•
    new_builder = KNNIndexBuilder(index_name=new_index)
    new_builder.create_knn_index()
    
    # 2. è¤‡è£½è³‡æ–™
    helpers.reindex(
        client=client,
        source_index=old_index,
        target_index=new_index
    )
    
    # 3. å»ºç«‹åˆ¥ååˆ‡æ›
    client.indices.update_aliases(
        body={
            "actions": [
                {"remove": {"index": old_index, "alias": "kb_alias"}},
                {"add": {"index": new_index, "alias": "kb_alias"}}
            ]
        }
    )
    
    # 4. åˆªé™¤èˆŠç´¢å¼•ï¼ˆå¯é¸ï¼‰
    # client.indices.delete(index=old_index)
```

## âš¡ æ•ˆèƒ½å„ªåŒ–

### æ‰¹æ¬¡å¤§å°å„ªåŒ–

```python
# æ ¹æ“šæ–‡ä»¶å¤§å°èª¿æ•´æ‰¹æ¬¡
def calculate_batch_size(avg_doc_size_kb):
    """è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
    if avg_doc_size_kb < 10:
        return 200
    elif avg_doc_size_kb < 50:
        return 100
    else:
        return 50
```

### ä¸¦è¡Œç´¢å¼•

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def parallel_index(documents, num_workers=4):
    """ä¸¦è¡Œç´¢å¼•æ–‡ä»¶"""
    # åˆ†å‰²æ–‡ä»¶
    chunk_size = len(documents) // num_workers
    chunks = [
        documents[i:i + chunk_size] 
        for i in range(0, len(documents), chunk_size)
    ]
    
    # ä¸¦è¡Œè™•ç†
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        tasks = []
        for chunk in chunks:
            task = asyncio.create_task(
                builder.bulk_index_documents(chunk)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
    
    return results
```

### è¨˜æ†¶é«”å„ªåŒ–

```python
# ä¸²æµè™•ç†å¤§æª”æ¡ˆ
async def stream_index_from_file(file_path, chunk_size=1000):
    """å¾æª”æ¡ˆä¸²æµç´¢å¼•"""
    documents = []
    
    with open(file_path, 'r') as f:
        for line in f:
            doc = json.loads(line)
            documents.append(doc)
            
            if len(documents) >= chunk_size:
                await builder.bulk_index_documents(documents)
                documents = []
        
        # è™•ç†å‰©é¤˜æ–‡ä»¶
        if documents:
            await builder.bulk_index_documents(documents)
```

## ğŸ” æ¸¬è©¦ç´¢å¼•å“è³ª

### æœå°‹æ¸¬è©¦

```bash
# åŸ·è¡Œæœå°‹æ¸¬è©¦
python scripts/test_knn_search.py --test accuracy

# æ¸¬è©¦ç‰¹å®šç­–ç•¥
python scripts/test_knn_search.py --test strategies
```

### å‘é‡ç›¸ä¼¼åº¦é©—è­‰

```python
# é©—è­‰å‘é‡å“è³ª
async def verify_vector_quality(sample_queries):
    """é©—è­‰å‘é‡æœå°‹å“è³ª"""
    for query in sample_queries:
        # KNN æœå°‹
        knn_results = await search_service.knn_search(
            query_text=query,
            strategy=SearchStrategy.KNN_ONLY
        )
        
        # BM25 æœå°‹
        bm25_results = await bm25_search_fn(query)
        
        # æ¯”è¼ƒçµæœé‡ç–Šåº¦
        knn_ids = {r.doc_id for r in knn_results[:5]}
        bm25_ids = {r.metadata['doc_id'] for r in bm25_results[:5]}
        
        overlap = len(knn_ids & bm25_ids) / 5
        print(f"æŸ¥è©¢: {query}")
        print(f"çµæœé‡ç–Šåº¦: {overlap * 100:.1f}%")
```

## ğŸ› ï¸ ç¶­è­·å»ºè­°

### å®šæœŸä»»å‹™

1. **æ¯æ—¥**
   - ç›£æ§ç´¢å¼•å¤§å°å’Œæ–‡ä»¶æ•¸é‡
   - æª¢æŸ¥æŸ¥è©¢å»¶é²

2. **æ¯é€±**
   - åˆ†ææ…¢æŸ¥è©¢
   - å„ªåŒ– HNSW åƒæ•¸

3. **æ¯æœˆ**
   - è©•ä¼°å‘é‡å“è³ª
   - è€ƒæ…®é‡å»ºç´¢å¼•

### ç›£æ§æŒ‡æ¨™

```python
# å»ºç«‹ç›£æ§æŒ‡æ¨™
metrics = {
    "index_size_mb": stats['size_in_bytes'] / 1024 / 1024,
    "doc_count": stats['document_count'],
    "avg_query_time_ms": 85,
    "cache_hit_rate": 0.75,
    "vector_dimension": stats['embedding_dimension']
}

# ç™¼é€åˆ° Prometheus
for metric, value in metrics.items():
    prometheus_gauge.labels(index=index_name).set(value)
```

## ğŸ”— ç›¸é—œè³‡æº

- [KNN å‘é‡æœå°‹æ¶æ§‹](../architecture/knn-vector-search.md)
- [KNN æœå°‹ API](../api/knn-search-api.md)
- [OpenSearch KNN å¤–æ›æ–‡æª”](https://opensearch.org/docs/latest/search-plugins/knn/)